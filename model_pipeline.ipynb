{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from normalize_text import normalize\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer, max_length):\n",
    "    norm_sents = [normalize(s) for s in sentences]\n",
    "    tokenized_sents = tokenizer(norm_sents, padding='max_length', truncation=True, max_length=max_length)\n",
    "    tokenized_sents = {k: torch.Tensor(v).long() for k, v in tokenized_sents.items()}\n",
    "    return tokenized_sents\n",
    "\n",
    "def get_sum_hidden_emb(tokenized_sents, model):\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(**tokenized_sents, output_hidden_states=True).hidden_states\n",
    "    stack_embs = torch.stack(hidden_states[-4:], dim=0) # use last 4 layers\n",
    "    embs = torch.sum(stack_embs, dim=0)\n",
    "\n",
    "    return torch.flatten(embs.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_list = [\n",
    "    r\"The shuttling of soluble sodium polysulfides (Na2Sn) and sluggish conversion kinetics are major roadblocks toward the practical realization of sodium–sulfur (Na–S) batteries. To undertake the challenges, we use first-principles calculations to design bifunctional electrocatalysts to achieve engineered interfaces with sulfur-based cathode materials. We illustrate the detailed behavior of Na2Sn adsorption, sulfur reduction reactions (SRRs), and catalytic decomposition on transition-metal (TM)-based single-atom catalysts (SACs) embedded on MoS2 substrates (SACs@MoS2). We observe that SACs doped on sulfur substitution and molybdenum top sites result in adequate binding energies to immobilize higher-order Na2Sn species. We found the d-band center as an important “descriptor” in dictating polysulfide adsorption energies and catalytic activities on SACs@MoS2. We elucidate that the larger upward shift of the d-band center toward the Fermi level and the involved higher number of vacant antibonding states are directly correlated to the adsorption strength of the Na2Sn. The V and Ni SACs are found to exhibit higher and lower binding energies, respectively, consistent with the d-band theory. Furthermore, the SACs that are electron-deficient sites demonstrate bifunctional electrocatalytic activity through reduced free energy for SRR and lower the barrier for Na2S decomposition in favor of accelerated electrode kinetics during discharge and charge processes, respectively. The electronic structure calculations reveal a significantly reduced band gap of the pristine and Na2Sn-adsorbed SACs@MoS2 due to mid-gap states, majorly stemming from TM-d orbitals, thus expected to improve the electronic conductivity of the substrates. The insight developed on the role of SACs in tailoring the polysulfides’ chemistry at the interfaces in relation to their d-band center is an important step toward the rational design of cathode materials for high-performance Na–S batteries.\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load Bert and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at m3rg-iitd/matscibert were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at m3rg-iitd/matscibert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained('m3rg-iitd/matscibert')\n",
    "model = AutoModel.from_pretrained('m3rg-iitd/matscibert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT setting\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "        for _ in range(num_layers-2):\n",
    "            self.linears.append(nn.Linear(hidden_dim, hidden_dim, bias=False))\n",
    "        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.batch_norm = nn.BatchNorm1d((hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.linears[:-1]:\n",
    "            h = F.relu(layer(self.dropout(h)))\n",
    "        # output layer\n",
    "        out = self.linears[-1](h)\n",
    "        return out\n",
    "    \n",
    "    def retrieve_emb(self, x):\n",
    "        h = x\n",
    "        for layer in self.linears[:-1]:\n",
    "            h = F.relu(layer(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load MLP\n",
    "n_layer = 4\n",
    "in_dim = 98304\n",
    "out_dim = 6\n",
    "hidden_dim = 128\n",
    "\n",
    "mlp = MLP(n_layer, in_dim, hidden_dim, out_dim, dropout=0)\n",
    "mlp.load_state_dict(torch.load(\"./mlp_models/080valacc.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_abs_list = [split_into_sentences(abs) for abs in abs_list]\n",
    "tokenized_list = [tokenize_sentences(split_abs, tokenizer, max_length) for split_abs in split_abs_list]\n",
    "emb_list = [get_sum_hidden_emb(tokenized_abs, model) for tokenized_abs in tokenized_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get MLP Augmented Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    aug_embs = mlp.retrieve_emb(torch.stack(emb_list, dim=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(embs_1, embs_2):\n",
    "    similarities = torch.mm(embs_1, embs_2.T)\n",
    "    norm_1 = torch.norm(embs_1, dim=1)\n",
    "    norm_2 = torch.norm(embs_2, dim=1)\n",
    "    norms = torch.mm(norm_1.view(-1, 1), norm_2.view(1, -1))\n",
    "    similarities = similarities / norms\n",
    "    similarities_dict = {}\n",
    "    for i in range(similarities.shape[0]):\n",
    "        similarities_dict[i] = similarities[i].tolist()\n",
    "\n",
    "    return similarities_dict\n",
    "\n",
    "def get_pair_ranks(sim_list):\n",
    "    sorted_id = sorted(range(len(sim_list)), key=lambda k: sim_list[k], reverse=True)\n",
    "\n",
    "    return sorted_id\n",
    "\n",
    "def create_similarities_df_with_doi(compared_df, emb_list_aim, emb_list_compared, K=None):\n",
    "    sim = get_similarities(emb_list_aim, emb_list_compared)\n",
    "    topk = {}\n",
    "\n",
    "    print(\"Calculating topk...\")\n",
    "    \n",
    "    if K is not None:\n",
    "        for key in sim.keys():\n",
    "            topk[key] = get_pair_ranks(sim[key])[:K]\n",
    "    else:\n",
    "        for key in sim.keys():\n",
    "            topk[key] = get_pair_ranks(sim[key])\n",
    "\n",
    "    print(\"Constructing df...\")\n",
    "    df_list = []\n",
    "    for key in topk.keys():\n",
    "        idx_length = len(emb_list_compared) if K is None else K\n",
    "        idx_list = [key for _ in range(idx_length)]\n",
    "        temp_df = compared_df[['class', 'title', 'doi']].iloc[topk[key]]\n",
    "        temp_df = temp_df.reset_index(drop= True)\n",
    "\n",
    "        sorted_sim = []\n",
    "        for id in topk[key]:\n",
    "            sorted_sim.append(sim[key][id])\n",
    "        sim_df = pd.DataFrame({'sim': sorted_sim})\n",
    "        id_df = pd.DataFrame({'id':idx_list})\n",
    "        df = pd.concat([id_df, temp_df, sim_df], axis=1)\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load abstracts for comparison\n",
    "aug_nas = torch.load('./embs/mlp_nas.pt')\n",
    "aug_sa = torch.load('./embs/mlp_sa.pt')\n",
    "aug_lis = torch.load('./embs/mlp_lis.pt')\n",
    "aug_lissa = torch.load('./embs/mlp_lissa.pt')\n",
    "aug_ir1 = torch.load('./embs/mlp_ir1.pt')\n",
    "aug_ir2 = torch.load('./embs/mlp_ir2.pt')\n",
    "aug_sup = torch.load(\"./embs/mlp_sup.pt\")\n",
    "\n",
    "aug_nassa = torch.load('./embs/mlp_nassa.pt')\n",
    "\n",
    "aug_compare_embs = torch.cat([aug_nas, aug_sa, aug_lis, aug_lissa, aug_ir1, aug_ir2, aug_sup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_with_class = pd.read_csv('./abs_data/all_abs_wo_nasa_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating topk...\n",
      "Constructing df...\n"
     ]
    }
   ],
   "source": [
    "# retrieve top-k\n",
    "k = 30\n",
    "topk_df = create_similarities_df_with_doi(all_df_with_class, aug_embs, aug_compare_embs, K=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matscibert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
