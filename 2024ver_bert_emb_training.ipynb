{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "from normalize_text import normalize\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "local_path = './bert-base-uncased/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_csv_file = \"./abs_data/TITLE-ABS-KEY ( room-temperature AND sodium-sulfur AND batteries ) AND ( DOCTYPE ( ar ) )_1.csv\"\n",
    "lis_csv_file = \"./abs_data/TITLE-ABS-KEY (  lithium-sulfur AND electrocatalysts) AND ( DOCTYPE ( ar ) )_1.csv\"\n",
    "sa_csv_file = \"./abs_data/new_data.csv\"\n",
    "lissa_csv_file = r\"./abs_data/Li-S SA_1.csv\"\n",
    "irrelevant1_csv_file = r'./abs_data/Low_relevant_1.csv'\n",
    "irrelevant2_csv_file = r'./abs_data/Low_relevant_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ir_file = r'./abs_data/24_new_ir_abs.csv'\n",
    "nir_df, nir_list = load_csv_abstracts(new_ir_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_df, nas_list = load_csv_abstracts(nas_csv_file)\n",
    "lis_df, lis_list = load_csv_abstracts(lis_csv_file)\n",
    "lissa_df, lissa_list = load_csv_abstracts(lissa_csv_file)\n",
    "sa_df, sa_list = load_csv_abstracts(sa_csv_file)\n",
    "\n",
    "ir1_df, ir1_list = load_csv_abstracts(irrelevant1_csv_file)\n",
    "ir2_df, ir2_list = load_csv_abstracts(irrelevant2_csv_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check duplicate abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df.shape, lis_df.shape, lissa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_duplicate_df = pd.concat([sa_df, lis_df, lissa_df])\n",
    "# check_duplicate_df.columns, check_duplicate_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_series = check_duplicate_df.duplicated(subset='doi', keep='last')\n",
    "# sa_check_series = check_series[:sa_df.shape[0]]\n",
    "# lis_check_series = check_series[sa_df.shape[0]:sa_df.shape[0] + lis_df.shape[0]]\n",
    "# lissa_check_series = check_series[-lissa_df.shape[0]:]\n",
    "# del check_duplicate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sa_check_series.value_counts())\n",
    "# sa_df[sa_check_series==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_duplicate(check_series, df, abs_list):\n",
    "#     print(check_series.value_counts())\n",
    "#     new_df = df[check_series==False]\n",
    "#     new_list = [] \n",
    "#     for check, abs in zip(check_series, abs_list):\n",
    "#         if check == False:\n",
    "#             new_list.append(abs)\n",
    "#     return new_df, new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sa_df, sa_list = remove_duplicate(sa_check_series, sa_df, sa_list)\n",
    "# lis_df, lis_list = remove_duplicate(lis_check_series, lis_df, lis_list)\n",
    "# lissa_df, lissa_list = remove_duplicate(lissa_check_series, lissa_df, lissa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> statistical info\")\n",
    "print(\"NaS: {nas}  |  LiS:  {lis}  |  Sa:  {sa}  |  LiSSA: {lissa}\".\n",
    "        format(nas=len(nas_list), lis=len(lis_list), sa=len(sa_list), lissa=len(lissa_list)))\n",
    "print(\"Irrelevant 1: {ir1}  |  Irrelevant 2: {ir2}\".format(ir1=len(ir1_list), ir2=len(ir2_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supplement data\n",
    "nssup_csv_file = r'./abs_data/ns_supplement.csv'\n",
    "npsup_csv_file = r'./abs_data/np_supplement.csv'\n",
    "othersup_csv_file = r'./abs_data/other_supplement.csv'\n",
    "\n",
    "nssup_df, nssup_list = load_csv_abstracts(nssup_csv_file)\n",
    "npsup_df, npsup_list = load_csv_abstracts(npsup_csv_file)\n",
    "othersup_df, othersup_list = load_csv_abstracts(othersup_csv_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sa_list = []\n",
    "for i, sa in enumerate(sa_list):\n",
    "    try:\n",
    "        tokenized_sa = tokenize_sentences(sa, tokenizer, 128)\n",
    "        tokenized_sa_list.append(tokenized_sa)\n",
    "    except IndexError as e:\n",
    "        print(\"IndexError  at {i}\".format(i=i))\n",
    "print(len(tokenized_sa_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sa_list = []\n",
    "for i, tokenized_sa in enumerate(tokenized_sa_list[:5000]):\n",
    "    tmp = get_sum_hidden_emb(tokenized_sa, model)\n",
    "    emb_sa_list.append(tmp)\n",
    "    if i % 100 == 0:\n",
    "        print(\"{i} / 5000\".format(i=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save emb\n",
    "torch.save(emb_sa_list, './bert_embs/24sa_list_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sa_list2 = []\n",
    "for i, tokenized_sa in enumerate(tokenized_sa_list[5000:]):\n",
    "    tmp = get_sum_hidden_emb(tokenized_sa, model)\n",
    "    emb_sa_list2.append(tmp)\n",
    "    if i % 100 == 0:\n",
    "        print(\"{i} / 5000\".format(i=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save emb\n",
    "torch.save(emb_sa_list2, './bert_embs/24sa_list_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del emb_sa_list\n",
    "del emb_sa_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if pre calculated\n",
    "# emb_nas_list = torch.load('./bert_embs/nas_list.pt')\n",
    "# emb_sa_list = torch.load('./bert_embs/24sa_list_2.pt')\n",
    "# emb_lis_list = torch.load('./bert_embs/lis_list.pt')\n",
    "# emb_lissa_list = torch.load('./bert_embs/24lissa_list.pt')\n",
    "# emb_ir1_list = torch.load('./bert_embs/24ir1_list.pt')\n",
    "# emb_ir2_list = torch.load('./bert_embs/24ir2_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate other emebeddings\n",
    "max_length=128\n",
    "tokenized_lissa_list = [tokenize_sentences(lissa, tokenizer, max_length) for lissa in lissa_list]\n",
    "tokenized_ir1_list = [tokenize_sentences(ir, tokenizer, max_length) for ir in ir1_list]\n",
    "tokenized_ir2_list = [tokenize_sentences(ir, tokenizer, max_length) for ir in ir2_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lissa_list = [get_sum_hidden_emb(tokenized_lissa, model) for tokenized_lissa in tokenized_lissa_list]\n",
    "del tokenized_lissa_list\n",
    "torch.save(emb_lissa_list, './bert_embs/24lissa_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_ir1_list = [get_sum_hidden_emb(tokenized_ir, model) for tokenized_ir in tokenized_ir1_list]\n",
    "del tokenized_ir1_list\n",
    "torch.save(emb_ir1_list, './bert_embs/24ir1_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_ir2_list = [get_sum_hidden_emb(tokenized_ir, model) for tokenized_ir in tokenized_ir2_list]\n",
    "# del tokenized_ir2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "base_path = './abs_data/'\n",
    "nas_sa_list = []\n",
    "for i in range(1, 6):\n",
    "    nas_sa_list.append(load_paper_to_sentences(base_path + 'nasa_{i}.txt'.format(i=i)))\n",
    "tokenized_nas_sa_list = [tokenize_sentences(nasa, tokenizer, max_length) for nasa in nas_sa_list]\n",
    "emb_nas_sa_list = [get_sum_hidden_emb(tokenized_nasa, model) for tokenized_nasa in tokenized_nas_sa_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieve supplement data\n",
    "# tokenized_nssup_list = [tokenize_sentences(nssup, tokenizer, max_length) for nssup in nssup_list]\n",
    "# tokenized_npsup_list = [tokenize_sentences(npsup, tokenizer, max_length) for npsup in npsup_list]\n",
    "# tokenized_othersup_list = [tokenize_sentences(othersup, tokenizer, max_length) for othersup in othersup_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_nssup_list = [get_sum_hidden_emb(tokenized_nssup, model) for tokenized_nssup in tokenized_nssup_list]\n",
    "# emb_npsup_list = [get_sum_hidden_emb(tokenized_npsup, model) for tokenized_npsup in tokenized_npsup_list]\n",
    "# emb_othersup_list = [get_sum_hidden_emb(tokenized_othersup, model) for tokenized_othersup in tokenized_othersup_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(emb_nssup_list, './embs/nssup_list.pt')\n",
    "# torch.save(emb_npsup_list, './embs/npsup_list.pt')\n",
    "# torch.save(emb_othersup_list, './embs/othersup_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_nir_list = [tokenize_sentences(nir, tokenizer, max_length=128) for nir in nir_list]\n",
    "# emb_nir_list = [get_sum_hidden_emb(tokenized_nir, model) for tokenized_nir in tokenized_nir_list]\n",
    "# torch.save(emb_nir_list, './embs/new_ir_list.pt')\n",
    "emb_nir_list = torch.load('./embs/new_ir_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_nssup_list = torch.load('./embs/nssup_list.pt')\n",
    "emb_npsup_list = torch.load('./embs/npsup_list.pt')\n",
    "emb_othersup_list = torch.load('./embs/othersup_list.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sup_title = [\n",
    "#     \"Tuning the Coordination Structure of CuNC Single Atom Catalysts for Simultaneous Electrochemical Reduction of CO2 and NO3– to Urea\",\n",
    "#     \"Fluorine-tuned single-atom catalysts with dense surface Ni-N4 sites on ultrathin carbon nanosheets for efficient CO2 electroreduction\",\n",
    "#     \"Insights on forming N,O-coordinated Cu single-atom catalysts for electrochemical reduction CO2 to methane\",\n",
    "#     \"Coordination-tuned Fe single-atom catalyst for efficient CO2 electroreduction: The power of B atom\",\n",
    "#     \"Axial coordination regulation of MOF-based single-atom Ni catalysts by halogen atoms for enhanced CO2 electroreduction\",\n",
    "#     \"Engineering unsymmetrically coordinated Cu-S1N3 single atom sites with enhanced oxygen reduction activity\",\n",
    "#     \"Tuning the Coordination Environment in Single-Atom Catalysts to Achieve Highly Efficient Oxygen Reduction Reactions\",\n",
    "#     \"Fe Isolated Single Atoms on S, N Codoped Carbon by Copolymer Pyrolysis Strategy for Highly Efficient Oxygen Reduction Reaction\"\n",
    "# ]\n",
    "\n",
    "# sup_doi = [\n",
    "#     \"10.1002/aenm.202201500\",\n",
    "#     \"10.1016/j.apcatb.2020.119591\",\n",
    "#     \"10.1038/s41467-020-20769-x\",\n",
    "#     \"10.1016/j.cej.2021.134270\",\n",
    "#     \"10.1007/s12274-022-4467-3\",\n",
    "#     \"10.1038/s41467-020-16848-8\",\n",
    "#     \"10.1021/jacs.9b09352\",\n",
    "#     \"10.1002/adma.201800588\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sup_abs = [\n",
    "#     r\"Closing both the carbon and nitrogen loops is a critical venture to support the establishment of the circular, net-zero carbon economy. Although single atom catalysts (SACs) have gained interest for the electrochemical reduction reactions of both carbon dioxide (CO2RR) and nitrate (NO3RR), the structure–activity relationship for Cu SAC coordination for these reactions remains unclear and should be explored such that a fundamental understanding is developed. To this end, the role of the Cu coordination structure is investigated in dictating the activity and selectivity for the CO2RR and NO3RR. In agreement with the density functional theory calculations, it is revealed that Cu-N4 sites exhibit higher intrinsic activity toward the CO2RR, whilst both Cu-N4 and Cu-N4−x-Cx sites are active toward the NO3RR. Leveraging these findings, CO2RR and NO3RR are coupled for the formation of urea on Cu SACs, revealing the importance of *COOH binding as a critical parameter determining the catalytic activity for urea production. To the best of the authors’ knowledge, this is the first report employing SACs for electrochemical urea synthesis from CO2RR and NO3RR, which achieves a Faradaic efficiency of 28% for urea production with a current density of −27 mA cm–2 at −0.9 V versus the reversible hydrogen electrode.\",\n",
    "#     r\"A fluorine-tuned single-atom catalyst with an ultrathin nanosheet morphology (only ∼1.25 nm) and high Ni content of 5.92 wt% was fabricated by a polymer-assisted pyrolysis approach. The synthetic approach not only controls the establishment of the ultrathin nanosheet structure for achieving high surface communication, but also incorporates F dopants to manipulate the electronic structure of the metalloporphyrin-like active sites (Ni-N4). As a result, such catalyst with unique structural features exhibits a remarkable electrocatalytic performance for CO2-to-CO conversion with the Faradaic efficiency (FE) over 95 % in a wide potential range and an outstanding CO evolution rate of 1146 mmol gcat–1 h–1 at –0.97 V vs. RHE. The in situ attenuated total reflection-infrared spectroscopy (ATR-IR) and theoretical calculations further demonstrate that the F-doping modulates the electron configuration of the central Ni-N4 sites and thereby reduces the energy barrier for CO2 activation, which is favorable to the generation of the key *COOH intermediate.\",\n",
    "#     r\"Single-atom catalysts (SACs) are promising candidates to catalyze electrochemical CO2 reduction (ECR) due to maximized atomic utilization. However, products are usually limited to CO instead of hydrocarbons or oxygenates due to unfavorable high energy barrier for further electron transfer on synthesized single atom catalytic sites. Here we report a novel partial-carbonization strategy to modify the electronic structures of center atoms on SACs for lowering the overall endothermic energy of key intermediates. A carbon-dots-based SAC margined with unique CuN2O2 sites was synthesized for the first time. The introduction of oxygen ligands brings remarkably high Faradaic efficiency (78%) and selectivity (99% of ECR products) for electrochemical converting CO2 to CH4 with current density of 40 mA·cm-2 in aqueous electrolytes, surpassing most reported SACs which stop at two-electron reduction. Theoretical calculations further revealed that the high selectivity and activity on CuN2O2 active sites are due to the proper elevated CH4 and H2 energy barrier and fine-tuned electronic structure of Cu active sites.\",\n",
    "#     r\"Designing of effective electrocatalysts for electrocatalytic CO2 reduction into value-added chemicals is the key to reducing CO2 concentration and achieving carbon neutrality. However, achieving high activity and product selectivity simultaneously remains a significant challenge. Herein, a series of Fe single-atom catalysts coordinated by B atoms, namely FeBxCy (x + y = 3 or 4), are constructed to systematically investigate the electrocatalytic CO2 reduction reaction (CO2RR) based on density functional theory computations. Eight catalysts, including FeO4, are identified that can effectively activate CO2 molecules and significantly inhibit competitive hydrogen evolution reaction (HER). Among them, FeB2C and FeB2C2h (h represents a cis structure) show the higher CO2RR activity with the less negative limiting potentials of −0.24 and −0.40 V toward production of CH4, indicating the optimal content for doping B atoms. The activity mechanism shows that d-band center and magnetic moment of central Fe atom can be manipulated by rational modulating the coordinated B atoms to improve the CO2RR performance. By the coordinated B atom, an optimal adsorption strength of the reaction intermediates can be achieved on the FeBxCy surface, and thereby increasing CO2RR catalytic activity and product selectivity. FeB2C with more negative d-band center and the optimal Fe atomic magnetic moment shows the best CO2RR performance. These results reveal a great potential of coordination tuning for CO2RR, and provide a new theoretical perspective for rational design of high activity, selective CO2RR catalysts.\",\n",
    "#     r\"Single-atom catalysts (SACs), with the utmost atom utilization, have attracted extensive interests for various catalytic applications. The coordination environment of SACs has been recognized to play a vital role in catalysis while their precise regulation at atomic level remains an immense challenge. Herein, a post metal halide modification (PMHM) strategy has been developed to construct Ni-N4 sites with axially coordinated halogen atoms, named Ni1N-C (X) (X = Cl, Br, and I), on pre-synthetic nitrogen-doped carbon derived from metal-organic frameworks. The axial halogen atoms with distinct electronegativity can break the symmetric charge distribution of planar Ni-N4 sites and regulate the electronic states of central Ni atoms in Ni1N-C (X) (X = Cl, Br, and I). Significantly, the Ni1N-C (Cl) catalyst, decorated with the most electronegative Cl atoms, exhibits Faradaic efficiency of CO up to 94.7% in electrocatalytic CO2 reduction, outperforming Ni1N-C (Br) and Ni1N-C (I) catalysts. Moreover, Ni1N-C (Cl) also presents superb performance in Zn-CO2 battery with ultrahigh CO selectivity and great durability. Theoretical calculations reveal that the axially coordinated Cl atom remarkably facilitates *COOH intermediate formation on single-atom Ni sites, thereby boosting the CO2 reduction performance of Ni1N-C (Cl). This work offers a facile strategy to tailor the axial coordination environments of SACs at atomic level and manifests the crucial role of axial coordination microenvironments in catalysis.\",\n",
    "#     r\"Atomic interface regulation is thought to be an efficient method to adjust the performance of single atom catalysts. Herein, a practical strategy was reported to rationally design single copper atoms coordinated with both sulfur and nitrogen atoms in metal-organic framework derived hierarchically porous carbon (S-Cu-ISA/SNC). The atomic interface configuration of the copper site in S-Cu-ISA/SNC is detected to be an unsymmetrically arranged Cu-S1N3 moiety. The catalyst exhibits excellent oxygen reduction reaction activity with a half-wave potential of 0.918 V vs. RHE. Additionally, through in situ X-ray absorption fine structure tests, we discover that the low-valent Cuprous-S1N3 moiety acts as an active center during the oxygen reduction process. Our discovery provides a universal scheme for the controllable synthesis and performance regulation of single metal atom catalysts toward energy applications.\",\n",
    "#     r\"Designing atomically dispersed metal catalysts for oxygen reduction reaction (ORR) is a promising approach to achieve efficient energy conversion. Herein, we develop a template-assisted method to synthesize a series of single metal atoms anchored on porous N,S-codoped carbon (NSC) matrix as highly efficient ORR catalysts to investigate the correlation between the structure and their catalytic performance. The structure analysis indicates that an identical synthesis method results in distinguished structural differences between Fe-centered single-atom catalyst (Fe-SAs/NSC) and Co-centered/Ni-centered single-atom catalysts (Co-SAs/NSC and Ni-SAs/NSC) because of the different trends of each metal ion in forming a complex with the N,S-containing precursor during the initial synthesis process. The Fe-SAs/NSC mainly consists of a well-dispersed FeN4S2 center site where S atoms form bonds with the N atoms. The S atoms in Co-SAs/NSC and Ni-SAs/NSC, on the other hand, form metal–S bonds, resulting in CoN3S1 and NiN3S1 center sites. Density functional theory (DFT) reveals that the FeN4S2 center site is more active than the CoN3S1 and NiN3S1 sites, due to the higher charge density, lower energy barriers of the intermediates, and products involved. The experimental results indicate that all three single-atom catalysts could contribute high ORR electrochemical performances, while Fe-SAs/NSC exhibits the highest of all, which is even better than commercial Pt/C. Furthermore, Fe-SAs/NSC also displays high methanol tolerance as compared to commercial Pt/C and high stability up to 5000 cycles. This work provides insights into the rational design of the definitive structure of single-atom catalysts with tunable electrocatalytic activities for efficient energy conversion.\",\n",
    "#     r\"Heteroatom-doped Fe-NC catalyst has emerged as one of the most promising candidates to replace noble metal-based catalysts for highly efficient oxygen reduction reaction (ORR). However, delicate controls over their structure parameters to optimize the catalytic efficiency and molecular-level understandings of the catalytic mechanism are still challenging. Herein, a novel pyrrole–thiophene copolymer pyrolysis strategy to synthesize Fe-isolated single atoms on sulfur and nitrogen-codoped carbon (Fe-ISA/SNC) with controllable S, N doping is rationally designed. The catalytic efficiency of Fe-ISA/SNC shows a volcano-type curve with the increase of sulfur doping. The optimized Fe-ISA/SNC exhibits a half-wave potential of 0.896 V (vs reversible hydrogen electrode (RHE)), which is more positive than those of Fe-isolated single atoms on nitrogen codoped carbon (Fe-ISA/NC, 0.839 V), commercial Pt/C (0.841 V), and most reported nonprecious metal catalysts. Fe-ISA/SNC is methanol tolerable and shows negligible activity decay in alkaline condition during 15 000 voltage cycles. X-ray absorption fine structure analysis and density functional theory calculations reveal that the incorporated sulfur engineers the charges on N atoms surrounding the Fe reactive center. The enriched charge facilitates the rate-limiting reductive release of OH* and therefore improved the overall ORR efficiency.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sup_dict = {'title': sup_title, 'abs': sup_abs, 'pub': None, 'doi': sup_doi, 'class': ['supplement' for _ in range(len(sup_doi))]}\n",
    "# sup_df = pd.DataFrame(sup_dict)\n",
    "# sup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sup_df.to_csv(\"./abs_data/supplied_abs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sup_list = []\n",
    "# for abs in sup_df['abs']:\n",
    "#     sup_list.append(split_into_sentences(abs))\n",
    "# tokenized_sup_list = [tokenize_sentences(sup, tokenizer, max_length) for sup in sup_list]\n",
    "# emb_sup_list = [get_sum_hidden_emb(tokenized_sup, model) for tokenized_sup in tokenized_sup_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(emb_sup_list, './embs/sup_list.pt')\n",
    "emb_sup_list = torch.load('./embs/sup_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save emb\n",
    "# torch.save(emb_lissa_list, './embs/lissa_list.pt')\n",
    "# torch.save(emb_ir1_list, './embs/ir1_list.pt')\n",
    "# torch.save(emb_ir2_list, './embs/ir2_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release memory\n",
    "del tokenizer, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Labeled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class AbstractDataset(Dataset):\n",
    "    def __init__(self, emb_nas_list, emb_lis_list, emb_sa_list, emb_lissa_list,\n",
    "                       emb_ir1_list, emb_ir2_list, emb_nssup_list, emb_npsup_list, emb_othersup_list) -> None:\n",
    "        # create labels\n",
    "        label_nas = torch.full([len(emb_nas_list) + len(emb_nssup_list) + len(emb_npsup_list) + len(emb_othersup_list)], 0)\n",
    "        label_lis = torch.full([len(emb_lis_list)], 1)\n",
    "        label_sa = torch.full([len(emb_sa_list)], 2)\n",
    "        label_lissa = torch.full([len(emb_lissa_list)], 3)\n",
    "        label_ir1 = torch.full([len(emb_ir1_list)], 4)\n",
    "        label_ir2 = torch.full([len(emb_ir2_list)], 5)\n",
    "        self.labels = torch.cat([label_nas, label_lis, label_sa, label_lissa, label_ir1, label_ir2])\n",
    "        # data\n",
    "        emb_nas = torch.stack(emb_nas_list)\n",
    "        emb_nssup = torch.stack(emb_nssup_list)\n",
    "        emb_npsup = torch.stack(emb_npsup_list)\n",
    "        emb_othersup = torch.stack(emb_othersup_list)\n",
    "        emb_lis = torch.stack(emb_lis_list)\n",
    "        emb_sa = torch.stack(emb_sa_list)\n",
    "        emb_lissa = torch.stack(emb_lissa_list)\n",
    "        emb_ir1 = torch.stack(emb_ir1_list)\n",
    "        emb_ir2 = torch.stack(emb_ir2_list)\n",
    "        self.data = torch.cat([emb_nas, emb_nssup, emb_npsup, emb_othersup, emb_lis, emb_sa, emb_lissa, emb_ir1, emb_ir2])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AbstractDataset(emb_nas_list, emb_lis_list, emb_sa_list, emb_lissa_list, emb_ir1_list, emb_ir2_list, emb_nssup_list, emb_npsup_list, emb_othersup_list)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def gen_train_test_idxs(offset, cls_idxs, test_rate=0, train_test_num={}):\n",
    "    if test_rate > 0:\n",
    "        train_idxs, test_idxs = train_test_split(cls_idxs, test_size=test_rate)\n",
    "    else:\n",
    "        train_idxs, test_idxs = train_test_split(cls_idxs, train_size = train_test_num['train'], test_size = train_test_num['test'])\n",
    "    print(\"Train size: {tr} | Test size: {te}\".format(tr=len(train_idxs), te=len(test_idxs)))\n",
    "    return [idx + offset for idx in train_idxs], [idx + offset for idx in test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each class has a maximun number of 300\n",
    "start_offset = 0\n",
    "cut_points = []\n",
    "train_idxs, val_idxs = [], []\n",
    "# nas\n",
    "temp_train, temp_test = gen_train_test_idxs(start_offset, list(range(len(emb_nas_list) + len(emb_nssup_list) + len(emb_npsup_list) + len(emb_othersup_list))), test_rate=0.1)\n",
    "train_idxs += temp_train\n",
    "val_idxs += temp_test\n",
    "cut_points.append([start_offset, start_offset + len(emb_nas_list) + len(emb_nssup_list) + len(emb_npsup_list) + len(emb_othersup_list)])\n",
    "start_offset += len(emb_nas_list)\n",
    "# lis\n",
    "temp_train, temp_test = gen_train_test_idxs(start_offset, list(range(len(emb_lis_list))), test_rate=0.1)\n",
    "train_idxs += temp_train\n",
    "val_idxs += temp_test\n",
    "cut_points.append([start_offset, start_offset + len(emb_lis_list)])\n",
    "start_offset += len(emb_lis_list)\n",
    "# sa\n",
    "temp_train, temp_test = gen_train_test_idxs(start_offset, list(range(len(emb_sa_list))), test_rate=0.3)\n",
    "train_idxs += temp_train\n",
    "val_idxs += temp_test\n",
    "cut_points.append([start_offset, start_offset + len(emb_sa_list)])\n",
    "start_offset += len(emb_sa_list)\n",
    "# lissa\n",
    "temp_train, temp_test = gen_train_test_idxs(start_offset, list(range(len(emb_lissa_list))), test_rate=0.1)\n",
    "train_idxs += temp_train\n",
    "val_idxs += temp_test\n",
    "cut_points.append([start_offset, start_offset + len(emb_lissa_list)])\n",
    "start_offset += len(emb_lissa_list)\n",
    "# ir1\n",
    "temp_train, temp_test = gen_train_test_idxs(start_offset, list(range(len(emb_ir1_list))), train_test_num={'train': 270, 'test':30})\n",
    "train_idxs += temp_train\n",
    "val_idxs += temp_test\n",
    "cut_points.append([start_offset, start_offset + len(emb_ir1_list)])\n",
    "start_offset += len(emb_ir1_list)\n",
    "# ir2\n",
    "temp_train, temp_test = gen_train_test_idxs(start_offset, list(range(len(emb_ir2_list))), train_test_num={'train': 270, 'test':30})\n",
    "train_idxs += temp_train\n",
    "val_idxs += temp_test\n",
    "cut_points.append([start_offset, start_offset + len(emb_ir2_list)])\n",
    "start_offset += len(emb_ir2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_idxs), len(val_idxs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train MLP for Augmented Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n",
    "        for _ in range(num_layers-2):\n",
    "            self.linears.append(nn.Linear(hidden_dim, hidden_dim, bias=False))\n",
    "        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.batch_norm = nn.BatchNorm1d((hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.linears[:-1]:\n",
    "            h = F.relu(layer(self.dropout(h)))\n",
    "        # output layer\n",
    "        out = self.linears[-1](h)\n",
    "        return out\n",
    "    \n",
    "    def retrieve_emb(self, x):\n",
    "        h = x\n",
    "        for layer in self.linears[:-1]:\n",
    "            h = F.relu(layer(h))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, name='', base='./mlp_models/'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.name = name\n",
    "        self.base = base\n",
    "\n",
    "    def step(self, acc, model):\n",
    "        score = acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        torch.save(model.state_dict(), self.base + self.name + '_checkpoint.pt')\n",
    "\n",
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    total_correct = 0\n",
    "    for batched_data, labels in dataloader:\n",
    "        total += len(labels)\n",
    "        logits = model(batched_data)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "    acc = 1.0 * total_correct / total\n",
    "    return acc\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, lr=0.01, verbose=True, stopper=None):\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "    loss_values = []\n",
    "    train_acc_values = []\n",
    "    val_acc_values = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, (batched_data, labels) in enumerate(train_loader):\n",
    "            logits = model(batched_data)\n",
    "            loss = loss_fcn(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        train_acc = evaluate(train_loader, model)\n",
    "        valid_acc = evaluate(val_loader, model)\n",
    "        loss_values.append(total_loss / (batch + 1))\n",
    "        train_acc_values.append(train_acc)\n",
    "        val_acc_values.append(valid_acc)\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "                    epoch, total_loss / (batch + 1), train_acc, valid_acc\n",
    "                )\n",
    "            )\n",
    "        if stopper is not None:\n",
    "            if stopper.step(valid_acc, model):\n",
    "                break\n",
    "    \n",
    "    return loss_values, train_acc_values, val_acc_values\n",
    "\n",
    "def train2(model, train_loader, val_loader, epochs, lr=0.01, verbose=True, stopper=None):\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "    loss_values = []\n",
    "    train_acc_values = []\n",
    "    val_acc_values = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, (batched_data, labels) in enumerate(train_loader):\n",
    "            logits = model(batched_data)\n",
    "            loss = loss_fcn(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # total_loss += loss.item()\n",
    "            train_acc = evaluate(train_loader, model)\n",
    "            valid_acc = evaluate(val_loader, model)\n",
    "            # loss_values.append(total_loss / (batch + 1))\n",
    "            loss_values.append(loss.item())\n",
    "            train_acc_values.append(train_acc)\n",
    "            val_acc_values.append(valid_acc)\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "                        epoch, loss.item(), train_acc, valid_acc\n",
    "                    )\n",
    "                )\n",
    "        scheduler.step()\n",
    "        # if stopper is not None:\n",
    "        #     if stopper.step(valid_acc, model):\n",
    "        #         break\n",
    "    \n",
    "    return loss_values, train_acc_values, val_acc_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "n_layer = 4\n",
    "in_dim = emb_sa_list[0].shape[0]\n",
    "out_dim = 6\n",
    "hidden_dim = 128\n",
    "\n",
    "name = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "early_stop = EarlyStopping(patience=10, name=name)\n",
    "\n",
    "# del mlp\n",
    "mlp = MLP(n_layer, in_dim, hidden_dim, out_dim, dropout=0.1)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_dataloader = DataLoader(dataset, sampler=SubsetRandomSampler(train_idxs), batch_size=256)\n",
    "test_dataloader = DataLoader(dataset, sampler=SubsetRandomSampler(val_idxs), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "loss_values, train_acc_values, val_acc_values = train2(mlp, train_dataloader, test_dataloader, epochs, stopper=early_stop, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp.state_dict(), './mlp_models/24new_mlp2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.load_state_dict(torch.load('./mlp_models/' + name +  '_checkpoint.pt'))\n",
    "evaluate(test_dataloader, mlp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(len(loss_values))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(epochs, loss_values, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', color=color)\n",
    "ax2.plot(epochs, train_acc_values, color=color, label='Train Accuracy')\n",
    "ax2.plot(epochs, val_acc_values, color=color, label='Validation Accuracy', linestyle='--')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Loss and Accuracy vs. Epochs')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.savefig('./save_files/loss_acc_plot.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./save_files/loss_acc_values.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Epoch', 'Loss', 'Train Accuracy', 'Validation Accuracy'])\n",
    "    for i in range(len(loss_values)):\n",
    "        writer.writerow([i, loss_values[i], train_acc_values[i], val_acc_values[i]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "feats, labels = dataset[val_idxs]\n",
    "logits = mlp(feats)\n",
    "_, preds = torch.max(logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_logits = logits.detach().softmax(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_recall_fscore_support(labels, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUROC\n",
    "metrics.roc_auc_score(labels, np_logits, multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aupr_labels = torch.zeros(labels.shape[0], 6)\n",
    "for i, label in enumerate(labels):\n",
    "    aupr_labels[i][label] = 1\n",
    "aupr_labels = aupr_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUPR\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(6):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(aupr_labels[:, i],\n",
    "                                                        np_logits[:, i])\n",
    "    average_precision[i] = average_precision_score(aupr_labels[:, i], np_logits[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(aupr_labels.ravel(),\n",
    "    np_logits.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(aupr_labels, np_logits,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.4f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision[\"micro\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_mat = confusion_matrix(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cf_mat).to_csv(\"./save_files/confusion_matrix.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save MLP embbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(dataset, cut_point):\n",
    "    x, _ = dataset[cut_point[0]: cut_point[1]]\n",
    "    return x\n",
    "def save_MLP_emb(mlp, dataset, cut_point, name):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        x = get_x(dataset, cut_point)\n",
    "        emb = mlp.retrieve_emb(x)\n",
    "        torch.save(emb, \"./embs/\" + name)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.load_state_dict(torch.load(\"./mlp_models/084valacc.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nas\n",
    "save_MLP_emb(mlp, dataset, cut_points[0], name=\"24mlp_nas.pt\")\n",
    "# lis\n",
    "save_MLP_emb(mlp, dataset, cut_points[1], name=\"24mlp_lis.pt\")\n",
    "# sa\n",
    "save_MLP_emb(mlp, dataset, cut_points[2], name=\"24mlp_sa.pt\")\n",
    "# lissa\n",
    "save_MLP_emb(mlp, dataset, cut_points[3], name=\"24mlp_lissa.pt\")\n",
    "# ir1\n",
    "save_MLP_emb(mlp, dataset, cut_points[4], name=\"24mlp_ir1.pt\")\n",
    "# ir2\n",
    "save_MLP_emb(mlp, dataset, cut_points[5], name=\"24mlp_ir2.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load trained MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer = 4\n",
    "in_dim = emb_nir_list[0].shape[0]\n",
    "out_dim = 6\n",
    "hidden_dim = 128\n",
    "\n",
    "mlp = MLP(n_layer, in_dim, hidden_dim, out_dim, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.load_state_dict(torch.load('./mlp_models/24new_mlp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_nir = torch.stack(emb_nir_list)\n",
    "print(emb_nir.shape)\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    emb = mlp.retrieve_emb(emb_nir)\n",
    "    torch.save(emb, './embs/24mlp_newir.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "mlp(emb_nir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save nassa emb\n",
    "emb_nassa = torch.stack(emb_nas_sa_list)\n",
    "emb_nassa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    emb = mlp.retrieve_emb(emb_nassa)\n",
    "    torch.save(emb, './embs/24mlp_nassa.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    emb_sup = torch.stack(emb_sup_list)\n",
    "    torch.save(mlp.retrieve_emb(emb_sup), './embs/24mlp_sup.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    emb_nssup = torch.stack(emb_nssup_list)\n",
    "    torch.save(mlp.retrieve_emb(emb_nssup), './embs/mlp_84_nssup.pt')\n",
    "    emb_npsup = torch.stack(emb_npsup_list)\n",
    "    torch.save(mlp.retrieve_emb(emb_npsup), './embs/mlp_84_npsup.pt')\n",
    "    emb_othersup = torch.stack(emb_othersup_list)\n",
    "    torch.save(mlp.retrieve_emb(emb_othersup), './embs/mlp_84_othersup.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('matscibert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd95327b7c7cceca9cf309281dbc71e99bfcc59fee14e06289becc9905c21812"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
