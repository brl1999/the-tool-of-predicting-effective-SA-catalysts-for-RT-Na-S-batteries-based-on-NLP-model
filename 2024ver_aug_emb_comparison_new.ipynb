{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Augmented Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_nas = torch.load('./embs/24mlp_nas.pt')\n",
    "aug_sa = torch.load('./embs/24mlp_sa.pt')\n",
    "aug_lis = torch.load('./embs/24mlp_lis.pt')\n",
    "aug_lissa = torch.load('./embs/24mlp_lissa.pt')\n",
    "aug_ir1 = torch.load('./embs/24mlp_ir1.pt')\n",
    "aug_ir2 = torch.load('./embs/24mlp_ir2.pt')\n",
    "aug_nassa = torch.load('./embs/mlp_nassa.pt')\n",
    "\n",
    "aug_sup = torch.load(\"./embs/24mlp_sup.pt\")\n",
    "\n",
    "# aug_nssup = torch.load(\"./embs/mlp_nssup.pt\")\n",
    "# aug_npsup = torch.load(\"./embs/mlp_npsup.pt\")\n",
    "# aug_othersup = torch.load(\"./embs/mlp_othersup.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aug_embs = torch.cat([aug_nas, aug_sa, aug_lis, aug_lissa, aug_ir1, aug_ir2, aug_sup, aug_nassa])\n",
    "aug_compare_embs = torch.cat([aug_nas, aug_sa, aug_lis, aug_lissa, aug_ir1, aug_ir2, aug_sup])\n",
    "embs_np = all_aug_embs.detach().numpy()\n",
    "all_aug_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nas = np.array(['nas' for _ in range(aug_nas.shape[0])])\n",
    "l_sa = np.array(['sa' for _ in range(aug_sa.shape[0])])\n",
    "l_lis = np.array(['lis' for _ in range(aug_lis.shape[0])])\n",
    "l_lissa = np.array(['lissa' for _ in range(aug_lissa.shape[0])])\n",
    "l_ir1 = np.array(['ir1' for _ in range(aug_ir1.shape[0])])\n",
    "l_ir2 = np.array(['ir2' for _ in range(aug_ir2.shape[0])])\n",
    "l_sup = np.array(['sup' for _ in range(aug_sup.shape[0])])\n",
    "l_nassa = np.array(['nassa' for _ in range(aug_nassa.shape[0])])\n",
    "# l_nssup = np.array(['nssup' for _ in range(aug_nssup.shape[0])])\n",
    "# l_npsup = np.array(['npsup' for _ in range(aug_npsup.shape[0])])\n",
    "# l_othersup = np.array(['othersup' for _ in range(aug_othersup.shape[0])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add supplement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_df = pd.read_csv(\"./abs_data/supplied_abs.csv\")\n",
    "nssup_df, nssup_list = load_csv_abstracts(\"./abs_data/ns_supplement.csv\")\n",
    "npsup_df, npsup_list = load_csv_abstracts(\"./abs_data/np_supplement.csv\")\n",
    "othersup_df, othersup_list = load_csv_abstracts(\"./abs_data/other_supplement.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_sup_df = pd.concat([nssup_df, npsup_df, othersup_df], axis=0)\n",
    "# new_sup_df.reset_index(inplace=True, drop=True)\n",
    "# l_nssup = np.array(['ns-sup' for _ in range(aug_nssup.shape[0])])\n",
    "# l_npsup = np.array(['np-sup' for _ in range(aug_npsup.shape[0])])\n",
    "# l_othersup = np.array(['other-sup' for _ in range(aug_othersup.shape[0])])\n",
    "# temp = pd.DataFrame(np.concatenate((l_nssup, l_npsup, l_othersup), axis=0), columns=['class'])\n",
    "# new_sup_df = pd.concat([new_sup_df, temp], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((l_nas, l_sa, l_lis, l_lissa, l_ir1, l_ir2, l_sup, l_nassa), axis=0)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_csv_file = \"./abs_data/TITLE-ABS-KEY ( room-temperature AND sodium-sulfur AND batteries ) AND ( DOCTYPE ( ar ) )_1.csv\"\n",
    "lis_csv_file = \"./abs_data/TITLE-ABS-KEY (  lithium-sulfur AND electrocatalysts) AND ( DOCTYPE ( ar ) )_1.csv\"\n",
    "# sa_csv_file = \"./abs_data/TITLE-ABS-KEY ( single-atom AND electrocatalysts ) AND ( DOCTYPE ( ar ) )_1.csv\"\n",
    "sa_csv_file = \"./abs_data/new_data.csv\"\n",
    "lissa_csv_file = r\"./abs_data/Li-S SA_1.csv\"\n",
    "irrelevant1_csv_file = r'./abs_data/Low_relevant_1.csv'\n",
    "irrelevant2_csv_file = r'./abs_data/Low_relevant_2.csv'\n",
    "nas_df, nas_list = load_csv_abstracts(nas_csv_file)\n",
    "lis_df, lis_list = load_csv_abstracts(lis_csv_file)\n",
    "lissa_df, lissa_list = load_csv_abstracts(lissa_csv_file)\n",
    "sa_df, sa_list = load_csv_abstracts(sa_csv_file)\n",
    "ir1_df, ir1_list = load_csv_abstracts(irrelevant1_csv_file)\n",
    "ir2_df, ir2_list = load_csv_abstracts(irrelevant2_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df = sa_df[:5000]\n",
    "sa_list = sa_list[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicate_df = pd.concat([sa_df, lis_df, lissa_df])\n",
    "check_series = check_duplicate_df.duplicated(subset='doi', keep='last')\n",
    "sa_check_series = check_series[:sa_df.shape[0]]\n",
    "lis_check_series = check_series[sa_df.shape[0]:sa_df.shape[0] + lis_df.shape[0]]\n",
    "lissa_check_series = check_series[-lissa_df.shape[0]:]\n",
    "del check_duplicate_df\n",
    "\n",
    "def remove_duplicate(check_series, df, abs_list):\n",
    "    print(check_series.value_counts())\n",
    "    new_df = df[check_series==False]\n",
    "    new_list = [] \n",
    "    for check, abs in zip(check_series, abs_list):\n",
    "        if check == False:\n",
    "            new_list.append(abs)\n",
    "    return new_df, new_list\n",
    "# sa_df, sa_list = remove_duplicate(sa_check_series, sa_df, sa_list)\n",
    "lis_df, lis_list = remove_duplicate(lis_check_series, lis_df, lis_list)\n",
    "lissa_df, lissa_list = remove_duplicate(lissa_check_series, lissa_df, lissa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.concat([nas_df, sa_df, lis_df, lissa_df, ir1_df, ir2_df], axis=0)\n",
    "# all_df.reset_index(inplace=True, drop=True)\n",
    "# print(all_df.shape)\n",
    "# all_df.to_csv(\"./abs_data/all_abs_wo_nasa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([nas_df, nssup_df, npsup_df, othersup_df, sa_df, lis_df, lissa_df, ir1_df, ir2_df, sup_df], axis=0)\n",
    "all_df.reset_index(inplace=True, drop=True)\n",
    "temp = pd.DataFrame(np.concatenate((l_nas, l_sa, l_lis, l_lissa, l_ir1, l_ir2, l_sup), axis=0), columns=['class'])\n",
    "all_df_with_class = pd.concat([all_df, temp], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_with_class.to_csv(\"./abs_data/all_abs_wo_nasa_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_with_class.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=100)\n",
    "embs_tsne = tsne.fit_transform(embs_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "predicts = kmeans.fit_predict(embs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(embs_tsne, columns=['dim1', 'dim2'])\n",
    "df_predicts = pd.DataFrame(predicts, columns=['cluster'])\n",
    "df_kmeans = pd.concat([df_temp, df_predicts], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_kmeans, hue='cluster', x='dim1', y='dim2', palette=\"Set3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(embs_tsne, columns=['dim1', 'dim2'])\n",
    "df_label = pd.DataFrame(labels, columns=['class'])\n",
    "df_tsne = pd.concat([df_temp, df_label], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne.to_csv(\"./24save_files/mlp_tsne_sup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_tsne, hue='class', x='dim1', y='dim2', palette='Set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d tsne\n",
    "tsne = TSNE(n_components=3)\n",
    "embs_tsne = tsne.fit_transform(embs_np)\n",
    "df_temp = pd.DataFrame(embs_tsne, columns=['dim1', 'dim2', 'dim3'])\n",
    "df_label = pd.DataFrame(labels, columns=['class'])\n",
    "df_tsne = pd.concat([df_temp, df_label], axis=1, ignore_index=False)\n",
    "df_tsne.to_csv(\"./save_files/mlp_tsne_3d.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "降维可视化结果表明：\n",
    "- 一方面，直接通过TSNE降维可视化的稳定性不强，每次降维结果不定，不好比较类间关系；\n",
    "- 另一方面，经过MLP编码后的embedding是类间距离增大，类内距离减少，符合使用MLP的初衷。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-K Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_similarities_df_with_doi(compared_df, emb_list_aim, emb_list_compared, K):\n",
    "#     topk = get_top_k(emb_list_aim, emb_list_compared, K)\n",
    "#     df_list = []\n",
    "#     for key in topk.keys():\n",
    "#         idx_list = [key for _ in range(K)]\n",
    "#         temp_df = compared_df[['class', 'title', 'doi']].iloc[topk[key]]\n",
    "#         temp_df = temp_df.reset_index(drop= True)\n",
    "#         id_df = pd.DataFrame({'id':idx_list})\n",
    "#         df = pd.concat([id_df, temp_df], axis=1)\n",
    "#         df_list.append(df)\n",
    "#     return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities_new(embs_1, embs_2):\n",
    "    similarities = torch.mm(embs_1, embs_2.T)\n",
    "    norm_1 = torch.norm(embs_1, dim=1)\n",
    "    norm_2 = torch.norm(embs_2, dim=1)\n",
    "    norms = torch.mm(norm_1.view(-1, 1), norm_2.view(1, -1))\n",
    "    similarities = similarities / norms\n",
    "    similarities_dict = {}\n",
    "    for i in range(similarities.shape[0]):\n",
    "        similarities_dict[i] = similarities[i].tolist()\n",
    "\n",
    "    return similarities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similarities_df_with_doi(compared_df, emb_list_aim, emb_list_compared, K=None):\n",
    "    sim = get_similarities_new(emb_list_aim, emb_list_compared)\n",
    "    topk = {}\n",
    "\n",
    "    print(\"Calculating topk...\")\n",
    "    \n",
    "    if K is not None:\n",
    "        for key in sim.keys():\n",
    "            topk[key] = get_pair_ranks(sim[key])[:K]\n",
    "    else:\n",
    "        for key in sim.keys():\n",
    "            topk[key] = get_pair_ranks(sim[key])\n",
    "\n",
    "    print(\"Constructing df...\")\n",
    "    df_list = []\n",
    "    for key in topk.keys():\n",
    "        idx_length = len(emb_list_compared) if K is None else K\n",
    "        idx_list = [key for _ in range(idx_length)]\n",
    "        temp_df = compared_df[['class', 'title', 'doi']].iloc[topk[key]]\n",
    "        temp_df = temp_df.reset_index(drop= True)\n",
    "\n",
    "        sorted_sim = []\n",
    "        for id in topk[key]:\n",
    "            sorted_sim.append(sim[key][id])\n",
    "        sim_df = pd.DataFrame({'sim': sorted_sim})\n",
    "        id_df = pd.DataFrame({'id':idx_list})\n",
    "        df = pd.concat([id_df, temp_df, sim_df], axis=1)\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all abstracts top-30\n",
    "aug_all_embs = torch.cat([aug_compare_embs, aug_nassa])\n",
    "compare_all_df = create_similarities_df_with_doi(all_df_with_class, aug_compare_embs, aug_compare_embs, k)\n",
    "compare_all_df.to_csv('./24save_files/all_sim_wo_nassa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_df = create_similarities_df_with_doi(all_df_with_class, aug_nassa, aug_compare_embs, k)\n",
    "compare_all_df.to_csv('./24save_files/aug_nassa_top30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_compare_embs_with_sup = torch.cat([aug_compare_embs, aug_sup])\n",
    "# aug_compare_embs_with_sup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df_with_sup = pd.concat([all_df, new_sup_df, sup_df], axis=0)\n",
    "all_df_with_sup = pd.concat([all_df, sup_df], axis=0)\n",
    "all_df_with_sup.reset_index(inplace=True, drop=True)\n",
    "all_df_with_sup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_compare_embs_with_sup = torch.cat([aug_compare_embs, aug_nssup, aug_npsup, aug_othersup, aug_sup])\n",
    "aug_compare_embs_with_sup = torch.cat([aug_compare_embs, aug_sup])\n",
    "aug_compare_embs_with_sup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_df_with_sup = create_similarities_df_with_doi(all_df_with_sup, aug_nassa, aug_compare_embs_with_sup, k)\n",
    "compare_all_df_with_sup.to_csv('./24save_files/new_aug_nassa_with_sup_top30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_df_with_sup = create_similarities_df_with_doi(all_df_with_sup, aug_nassa, aug_compare_embs_with_sup, K=10)\n",
    "compare_all_df_with_sup.to_csv('./24save_files/new_aug_nassa_with_sup_top10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_df_with_sup = create_similarities_df_with_doi(all_df_with_sup, aug_nassa, aug_compare_embs_with_sup, K=50)\n",
    "compare_all_df_with_sup.to_csv('./24save_files/new_aug_nassa_with_sup_top50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_all_df_with_sup = create_similarities_df_with_doi(all_df_with_sup, aug_nassa, aug_compare_embs_with_sup)\n",
    "compare_all_df_with_sup.to_csv('./24save_files/new_aug_nassa_with_sup_all.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = get_similarities(aug_nassa, aug_compare_embs_with_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_id = {}\n",
    "for key in sim.keys():\n",
    "    sorted_id[key] = get_pair_ranks(sim[key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('matscibert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd95327b7c7cceca9cf309281dbc71e99bfcc59fee14e06289becc9905c21812"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
